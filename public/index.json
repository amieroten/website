[{"authors":["admin"],"categories":null,"content":"Hello! My name is Amie Roten, and I am currently a Master\u0026rsquo;s student studying Computer Science at Oregon Health and Science University (OHSU) in Portland, OR. I originally hail from Central Illinois, and received my undergraduate degree in Linguistics from the University of Illinois at Urbana-Champaign. Prior to pursuing my degree in CS, I spent time working as a Research Assistant in a speech perception lab at the National Center for Rehabilitative Auditory Research at the Portland V.A., and became increasingly interested in how humans perceive sound, particularly speech, both in ideal circumstances and not-so-ideal. I\u0026rsquo;ve had the opportunity to extend this knowledge during my time at OHSU by applying it to automatic speech recognition problems, and am particularly fascinated by the overlap in human and machine \u0026lsquo;hearing\u0026rsquo;.\nI am also always ready and willing to discuss the subtle boundaries between cups and mugs. :)\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vigilant-golick-64a677.netlify.app/author/amie-roten/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/amie-roten/","section":"authors","summary":"Hello! My name is Amie Roten, and I am currently a Master\u0026rsquo;s student studying Computer Science at Oregon Health and Science University (OHSU) in Portland, OR. I originally hail from Central Illinois, and received my undergraduate degree in Linguistics from the University of Illinois at Urbana-Champaign.","tags":null,"title":"Amie Roten","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"https://vigilant-golick-64a677.netlify.app/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"  A preliminary note: If the visualization above does not render well on your machine, please visit the application directly, at https://amie-roten.shinyapps.io/Data_Exploration_Phonological_Features/ !\nOverview The interactive visualization above was created primarily as a final project submission for OHSU’s Data Visualization course, but also as a tool to guide investigation of data from a research project I’ve been working on during my time at OHSU. So, in that sense, this is a bit of a version 1.0, and I plan to keep modifying and updating it as different analyses are required, or certain analyses are found to be more or less usful than others! In addition to building the visualization for these purposes, I also wanted an excuse to play around more with R’s Shiny package, as I think it’s a very cool tool for building interesting, dynamic visualizations!\nFor a brief overview, please to check out a short presentation about the visualization below:\n\u0026lt; video here \u0026gt;\nIf you’d like more specific, in-depth details, including how I went about building this visualization, read on!\n Data As mentioned, the data used to build this visualization comes from a research project I’ve been a part of for the last academic year, in which we’ve built a prototype automatic pronunciation assessment system designed for children with speech sound disorders. The system takes in speech as an audio signal, maps each individual frame to a phoneme (aka speech sound, such as ‘sh’/ʃ) as well as a set of phonological features, for example, “voiced”, “coronal”, etc. Then, the mapped phonological features are passed through an additional model, which predicts whether the uttered phoneme from which the feature predictions came was correctly or incorrectly pronounced. In addition to the binary pronunciation judgement, the model also predicts the most-likely phonological feature error contributing to a phoneme being deemed mispronounced, to ultimately be used as corrective feedback for the user of the system, e.g. “this phoneme was unvoiced!”.\nAdditionally, for both overall mispronounciation detection and feedback, the user can select a threshold used to determine whether to pass judgement on a particular phoneme. That is, while the system can provide a binary judgement in the form of a zero (mispronounced) or one (correctly pronounced) on whether the phoneme was mispronounced for all input, the raw data output by the model is in the form of a probability of whether the phoneme was correctly pronounced, which we refer to as a “confidence level”. The higher the threshold chosen, the more selective we can be about which phonemes we deliver feedback on. For example, the system may report a phoneme as mispronounced, but if it is only 20% confident, we may not want to deliver feedback to the user, as users can be discouraged and confused if they are told that they have mispronounced a sound when they have not. The confidence level for the feedback assigned can also be adjusted.\nAs you can tell, this model generates quite a lot of data during various phases, and metrics to evaluate the performance of the model, such as accuracy, can differ based on how the data is thresholded. In addition, we evaluated the model using both typically-developing (TD) and speech-disordered (SD) children’s speech, so it would be handy to be able to quickly take a look at how the model performance may differ depending on group. Finally, we assessed the model’s performance using data that has been manually segmented into phonemes, as well as data that was force-aligned. We expect that performance would differ based on the segmentation method as well.\nWith all of these ways to drill down into the data, I wanted to create a dashboard where the user could ask a question about the dataset, quickly select the subset of interest, and get a report of the model’s performance for that particular subset. After learning about R’s Shiny library in OHSU’s Data Visualization class, I figured this would be an excellent tool for creating a dashboard to explore this dataset! Spoiler alert – it is!\nI’ve included a glimpse of the dataframe generated by the model below, demonstrating just how much information we were dealing with:\ndata \u0026lt;- read_csv(\u0026quot;data/phono_data_FA.csv\u0026quot;) %\u0026gt;% mutate(X1 = NULL) glimpse(data) ## Rows: 17,099 ## Columns: 11 ## $ actual_phn \u0026lt;chr\u0026gt; \u0026quot;d\u0026quot;, \u0026quot;ʌ\u0026quot;, \u0026quot;k\u0026quot;, \u0026quot;k\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;æ\u0026quot;, \u0026quot;k\u0026quot;, \u0026quot;sʲ\u0026quot;,… ## $ expected_phn \u0026lt;chr\u0026gt; \u0026quot;d\u0026quot;, \u0026quot;ʌ\u0026quot;, \u0026quot;k\u0026quot;, \u0026quot;k\u0026quot;, \u0026quot;w\u0026quot;, \u0026quot;æ\u0026quot;, \u0026quot;k\u0026quot;, \u0026quot;s\u0026quot;, … ## $ actual_score \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0… ## $ predicted_score \u0026lt;dbl\u0026gt; 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1… ## $ predicted_score_conf \u0026lt;dbl\u0026gt; 0.06636703, 0.33367300, 0.33663034, 0.29… ## $ actual_top_error \u0026lt;chr\u0026gt; \u0026quot;delrel\u0026quot;, \u0026quot;back\u0026quot;, \u0026quot;cor\u0026quot;, \u0026quot;cor\u0026quot;, \u0026quot;round\u0026quot;,… ## $ predicted_top_error \u0026lt;chr\u0026gt; \u0026quot;delrel\u0026quot;, \u0026quot;back\u0026quot;, \u0026quot;pause\u0026quot;, \u0026quot;delrel\u0026quot;, \u0026quot;ro… ## $ predicted_top_error_conf \u0026lt;dbl\u0026gt; 0.9450728, 0.7203612, 0.2179701, 0.19645… ## $ predicted_top_error_validity \u0026lt;lgl\u0026gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,… ## $ word \u0026lt;chr\u0026gt; \u0026quot;duck\u0026quot;, \u0026quot;duck\u0026quot;, \u0026quot;duck\u0026quot;, \u0026quot;quack\u0026quot;, \u0026quot;quack\u0026quot;… ## $ dx \u0026lt;chr\u0026gt; \u0026quot;SD\u0026quot;, \u0026quot;SD\u0026quot;, \u0026quot;SD\u0026quot;, \u0026quot;SD\u0026quot;, \u0026quot;SD\u0026quot;, \u0026quot;SD\u0026quot;, \u0026quot;SD\u0026quot;… As you can see, each row corresponds to an actual, uttered phoneme, and includes attributes such as what phoneme was expected to be produced, what the human-assigned score was, what the machine predicted score was, and so on.\n Audience My goal was to create a way for a user to dig deeper into this dataset than what a single, static visualization could allow. At this point, the model is still in prototype mode, and there are many questions to be answered and knobs to tweak before unleashing this system on the world. So, I designed the dashboard specifically with members of the lab in mind, as well as potentially outside researchers who have dealt with this type of system and data before. Admittedly, this is a visualization with a lot more information on it than most not associated with this project would need, or even want. However, given the stage of the project, including as much information as possible was my goal, in order to uncover any potential problems or nuances of the system to continue to tweak during development.\nI would like to rethink the visualization at some point to be more digestable for a general audience, as I’m pretty proud of this project and would love to share it with a wider set of people. However, at this point, a highly-detailed dashboard with as much information as possible (while still being as aesthetically-pleasing and non-chaotic as could be) was my goal!\n Type As far as categorizing this visualization, it’s a bit difficult to narrow it down to one, particular type of graph/chart. If forced, I’d say it’s a “data dashboard”, of the like often seen these days to share information about the ongoing pandemic, or which might be used by a company to monitor various aspects of their business. These are often created so that a user can explore a large dataset, or multiple datasets, and find specific answers to targeted questions.\nThe overall dashboard contains a number of different types of visualizations, including confusion matrices, tables, and a bar chart. I aimed to keep the individual visualizations simple and straightforward to take in, considering there are multiple on the same page. While tables and bar charts are relatively common, straightforward methods of plotting data, confusion matrices may be a bit less well-known. While these visualizations are used in a number of research/academic settings, they are perhaps most often used to describe the performance of a classification algorithm, as in the case of this visualization. Although we can describe performance of a system in terms of overall accuracy, a confusion matrix allows the reader to assess system performance for each particular group/class. For example, we may want to know how well this system accurately identifies incorrectly pronounced phonemes, or how often phonemes pronounced correctly are falsely labeled incorrect. A confusion matrix helps answer these questions in a (more-or-less) straight-forward manner by plotting each possible ground-truth and predicted label combination, and giving the counts for each combination. The left-upper to right-lower diagonal indicates correct predictions, and the other boxes are incorrect predictions. Although it may take some practice to get familar with these visualizations, they’re very handy for fine-grained assessments of a classifier’s performance!\n Reader’s Guide If you’ve made it this far, I’ll assume that you’re interested in getting a bit more detail about just how to use this visualization! Although I mentioned that my audience is primarily those already familiar with the data, this is not entirely true…with a bit of explaination, I think anyone could play around with this dashboard and make some interesting observations.\nSo, with the basics of the data as described above in mind, I’ll begin by describing each element of the dashboard, starting from the left column and continuing counterclockwise. For now, please ignore the sliders and buttons (I know, it’s hard!).\nBasic Metrics   The element above is pretty straightforward – this is just some text that reports a couple of basic metrics about the data. The most basic, perhaps, is the overall mispronunciation detection accuracy, although it is important to note that we use balanced accuracy for this dataset. That is, instead of computing it using the typical accuracy formula:\n\\[accuracy=\\frac{total\\text{ }correct}{total\\text{ }examples}\\]\n…it is calculated using the average of the accuracies for each individual class, as in the formula below:\n\\[balanced\\text{ }accuracy=\\frac{1}{2}\\left(\\frac{total\\text{ }mispronunciations\\text{ }predicted\\text{ } correctly}{total\\text{ }mispronunciations}+\\frac{total\\text{ }correct\\text{ }pronunciations\\text{ }predicted\\text{ } correctly}{total\\text{ }correct\\text{ }pronunciations}\\right)\\]\nWow, that’s a wordy equation! For a less-specific, more technical discussion of balanced accuracy, please check out this article.\nBasically, this is used when a dataset has an imbalanced number of examples in each class, so that the accuracy of the model on one particular class does not overwhelm the others. We have a lot more correct pronunciations than mispronunciations in this dataset, which is why this is the metric we are using!\nThe basic metrics section also reports on how many phonemes are assigned scores, therefore are used in the calculation of accuracy. The more strict a threshold chosen, the fewer phonemes are assigned scores…I will discuss this a bit more when I review the sliders and buttons. The number on the left is the total phonemes scored, and on the right is the total number of phonemes in the dataset, giving the proportion of how many phonemes were scored.\nThe dashboard also reports (standard) accuracy for the phonological feature feedback. This is a bit complicated, but essentially, each phoneme has a set of expected phonological features expected to be present when that phoneme is realized in speech. A pronunciation error occurs when a realized phoneme does not match the phoneme expected, for example, someone may “fum” instead of “thumb”, substituting the “f” sound for the “th” sound. In this case, there are certain phonological features expected to be present or absent in the “th” sound that differ from the expectations for the “f” sound. If the system predicts a top error as one of the discrepancies between the actual and expected phoneme, we consider that a “correct” prediction, otherwise it is incorrect. From this information, we can calculate the feedback accuracy, however, it is important to note that feedback performance is only assessed for mispronounced phonemes which the system correctly identifies as mispronounced. In addition, the proportion of these phonemes that are assigned feedback is reported, in the same fashion as the proportion of phonemes assigned scores.\n Binary Confusion Matrix   Although the confusion matrix was discussed briefly above, I’ll give a few more details here of how to go about reading this particular example. As labeled, the \\(x\\)-axis corresponds to correct, ground-truth labels, and the \\(y\\)-axis to the model-predicted labels. So, the top-left box corresponds to true negatives, or mispronounced phonemes correctly identified as mispronounced, whereas the bottom-left box corresponds to false positives, or mispronounced phonemes falsely identified as correctly pronounced. The upper-right box corresponds to false negatives, and the lower-right to true positives, defined as correctly pronounced phonemes mislabeled and correctly labeled, respectively.\nIn this particular example, we can see that the majority of the examples are correctly labeled, however we do have 125 false positives, and 760 false negatives. The model still has room for improvement!\n Phonological Feature Confusion Matrix   Wow, this is quite the figure! The general idea behind this figure is exactly the same as the figure depicted/described in the previous section, only we have many, many more classes here. Again, the upper-left to bottom-right diagonal corresponds to top phonological feature errors that were correctly identified by the model, and the other boxes to incorrect guesses. As we can see (if we squint!), we can make a number of observations about the model’s performance using this plot, for example, for the “coronal” feature, the system identifies this correctly a good portion of the time (908 times, to be exact), but does rather commonly mistakenly predict “lateral” as the top error when it should be “coronal”. Does this make sense? Well, we can consider what these features really mean: “coronal” refers to a sound which is articulated using the tip/blade of the tongue, specifically referring to the place of articulation, and “lateral” refers to sounds in which the center of the tongue contacts the roof of the mouth, so the flow air is forced through the sides of the vocal tract. So, it does seem reasonable that the model would get these somewhat similar qualities “confused”…interesting! As you can see, while this figure has a lot going on, close inspection can be rewarded with fascinating insights.\n Feature-Level Accuracy for Phonological Feedback   One may also be interested the overall accuracy of the for individual phonological features…well, here is the plot to answer all of those questions! This is a fairly simple bar chart, where each phonological feature that was predicted as an error in the current data subset is on the \\(x\\)-axis, and the percentage of instances where it was correctly predicted is on the \\(y\\)-axis. Using this plot, we can carefully determine whether the system performs better on certain phonological features than others, which may inform how to tweak the system or balance the input data in the future. We can see here that the system performs well on the “lateral” feature, but not so well on the “round” feature.\nFor this type of analysis, it’s also important to know how many times each feature occured – a feature with 100% accuracy, but only one example, tells us much less than a feature with 97% accuracy over 100 examples. So, the frequency of each feature is listed under it’s label, and the features are arranged going left-to-right from most to least occurances.\n Top Substitutions   Finally, there are a couple of tables reporting on the most frequent, specific phoneme substitutions in the dataset, as well as how accurately the model predicts them to be mispronounced. This, like the phonological feature confusion matrix, allows for detailed analyses of the model’s performance, for example, that commonly occuring substitutions are, fortunately, correctly identified most, if not all, of the time. It is important, though, to examine pairs for which the model does not perform well, as can be seen in the table on the right. Although some of these phonemes are quite seldom-occuring, it’s interesting to note that three of the pairs are vowel errors, and two are nasal consonont errors. Perhaps these are difficult problems for the model, in a larger sense. Certainly, this information can guide further analyses and questions about the model’s performance, and the data itself.\n Now, to click the buttons and move the dials! Finally, the interactive part!\n  There are several elements that can be adjusted in this dashboard, to examine certain subsets of the data. First, there are two sliders that can be used to adjust the confidence level thresholds used to determine which phonemes to deliver binary feedback on, as well as which mispronounciations to provide feature feedback for. As alluded to previously, the higher the threshold value, the stricter the system is, and the fewer phonemes are assigned feedback.\nThis thresholding impacts the entire dashboard, as it limits the data used to generate the figures to just the phonemes that satisfy these threshold requirements. As expected, a lax mispronunciation detection threshold of 0.2, or a minimum required confidence of 20%, will assign feedback to many phonemes, but may result in lower accuracy, whereas a strict threshold of 0.7 will only assign feedback for examples which the system is 70% “confident” in its decision (please excuse the anthropomorphization!), hopefully resulting in higher accuracy. Similar logic applies for the feedback assignment thresholding. Playing with different combinations help the user uncover yet more about the behavior of the system, and how it may be best utilized in practice.\n  Finally, the user can decide whether they would like to work with the force-aligned, or manually segmented data, as well as subset it based on speaker’s diagnosis (again, TD refers to “typically-developing” and SD to \u0026quot;speech-disordered).\nI encourage you to play around with the data, and please let me know if you uncover anything particularly interesting!\n  Aesthetic Choices My main goal when considering the aesthetics of the visualization was to make it as calm and easy to digest as possible, to offset the large amount and wide variety of information present. Although we learned a lot of neat tricks in this class to customize color palettes and fonts, I ended up taking a “less is more” approach in this particular case.\nSpecifically, I decided to continue with Shiny’s default color and font choices, and propogate them into the rest of the content. For the most part, this didn’t require much modification, other than changing some font sizes in the figures to better match those in the Shiny h*() elements, and pulling the pretty default blue used in the Shiny sliders/buttons into the plots.\nThe tricker part was arranging the widgets/plots in the overall dashboard. I spent a lot of time thinking through the best/most aesthetically pleasing way to arrange everything on the page, which had to be torn down and built back up several times as I added elements. I ended up arranging the page into three main sections; a bar across the top with the title, description, and toggles, the higher-level elements, and two evenly-sized columns containing the analyses. Having as few invisible “lines” breaking up the elements as possible really helped calm down the overall visual effect, in my opinion. I wish I had more to say about aesthetics, as it was something I put a lot of thought in to, but it manifested in a very simple way! I suppose I did use the lovely “theme_minimal()” option on my plots, but otherwise, I kept it clean and basic for a reason.\n Methods Now, on to methods. I began by contemplating what I wanted to include in the dashboard, and then I proceeded to dive right in, without thinking too much about aesthetics or layout. I ended up with an early version that looked something like this:\n  I wasn’t happy with the layout at this point, it felt sort of uneven and clunky, and it was starting to get a little bit tricky to continue adding on, knowing that I would have to rearrange eventually. It’s a somewhat challenging to arrange widgets in a Shiny application, especially without having a very solid plan of how the final layout would be, so I went, quite literally, back to the drawing board:\n  I knew I wanted to have an even split down the middle of the page as much as possible, with a cohesive bar across the top with the higher-level details/interactive elements. Drawing it out really helped plan how to set up the fluidRow() functions in the ui() portion below, as some nesting was required. Thinking and planning it out before coding it up made the process much smoother.\nAlthough I did end up switching around the placement of a few elements, the final product’s layout ended up very similar to my outlined vision!\nI’ve included the code corresponding to the final version below, with a few comments and details on a few of the more perplexing steps:\nlibrary(shiny) library(gt) library(ggpubr) library(tidyverse) library(ggplot2) library(comprehenr) library(yardstick) feature_mapper \u0026lt;- function(abbreviation) { full \u0026lt;- case_when(abbreviation == \u0026#39;delrel\u0026#39; ~ \u0026quot;Delayed Release\u0026quot;, abbreviation == \u0026#39;back\u0026#39; ~ \u0026quot;Back\u0026quot;, abbreviation == \u0026#39;cor\u0026#39; ~ \u0026quot;Coronal\u0026quot;, abbreviation == \u0026#39;round\u0026#39; ~ \u0026quot;Round\u0026quot;, abbreviation == \u0026#39;lo\u0026#39; ~ \u0026quot;Low\u0026quot;, abbreviation == \u0026#39;voi\u0026#39; ~ \u0026quot;Voiced\u0026quot;, abbreviation == \u0026#39;hi\u0026#39; ~ \u0026quot;High\u0026quot;, abbreviation == \u0026#39;son\u0026#39; ~ \u0026quot;Sonorant\u0026quot;, abbreviation == \u0026#39;syl\u0026#39; ~ \u0026quot;Syllabic\u0026quot;, abbreviation == \u0026#39;labiodental\u0026#39; ~ \u0026quot;Labiodental\u0026quot;, abbreviation == \u0026#39;tense\u0026#39; ~ \u0026quot;Tense\u0026quot;, abbreviation == \u0026#39;cont\u0026#39; ~ \u0026quot;Continuant\u0026quot;, abbreviation == \u0026#39;pause\u0026#39; ~ \u0026quot;Pause\u0026quot;, abbreviation == \u0026#39;ant\u0026#39; ~ \u0026quot;Anterior\u0026quot;, abbreviation == \u0026#39;lat\u0026#39; ~ \u0026quot;Lateral\u0026quot;, abbreviation == \u0026#39;front\u0026#39; ~ \u0026quot;Front\u0026quot;, abbreviation == \u0026#39;nas\u0026#39; ~ \u0026quot;Nasal\u0026quot;, abbreviation == \u0026#39;cons\u0026#39; ~ \u0026quot;Consonantal\u0026quot;, abbreviation == \u0026#39;lab\u0026#39; ~ \u0026quot;Labial\u0026quot;, abbreviation == \u0026#39;distr\u0026#39; ~ \u0026quot;Distributed\u0026quot;, abbreviation == \u0026#39;strid\u0026#39; ~ \u0026quot;Strident\u0026quot;, abbreviation == \u0026#39;sg\u0026#39; ~ \u0026quot;Spread Glottis\u0026quot;) return(full) } # Read in both datasets. Could do this dynamically, # but then the datasets would be repeatedly read in. # This way it is only done once. data_child_fa \u0026lt;- read.csv(\u0026quot;data/phono_data_FA.csv\u0026quot;) %\u0026gt;% mutate(actual_phn = as.character(actual_phn), expected_phn = as.character(expected_phn), actual_top_error = as.character(actual_top_error), predicted_top_error = as.character(predicted_top_error)) %\u0026gt;% mutate(actual_top_error = feature_mapper(actual_top_error), predicted_top_error = feature_mapper(predicted_top_error)) %\u0026gt;% mutate(actual_top_error = as.factor(actual_top_error), predicted_top_error = as.factor(predicted_top_error)) data_child_manual \u0026lt;- read.csv(\u0026quot;data/phono_data_manual.csv\u0026quot;) %\u0026gt;% mutate(actual_phn = as.character(actual_phn), expected_phn = as.character(expected_phn), actual_top_error = as.character(actual_top_error), predicted_top_error = as.character(predicted_top_error)) %\u0026gt;% mutate(actual_top_error = feature_mapper(actual_top_error), predicted_top_error = feature_mapper(predicted_top_error)) %\u0026gt;% mutate(actual_top_error = as.factor(actual_top_error), predicted_top_error = as.factor(predicted_top_error)) # Define UI for application ui \u0026lt;- fluidPage( titlePanel(\u0026quot;Data Exploration for Mispronunciation Detection System\u0026quot;), fluidRow( column(6, h5(\u0026quot;This module allows a user to explore performance metrics for a prototype automatic mispronunciation detection system. The system scores individual phonemes on overall pronunciation (correct/incorrect), and assigns corrective feedback based on the most-likely phonological feature error. The confidence threshold used to determine whether to assign an overall score and feedback can be adjusted using the sliders on the right, higher values corresponding to higher confidence in the assessment. Phoneme alignment method and speaker diagnosis (TD: typically developing, SD: speech disordered) can also be toggled.\u0026quot;)), column(2, sliderInput(\u0026quot;threshold_binary\u0026quot;, \u0026quot;Mispronunciation Detection:\u0026quot;, min=0.0, max=0.90, value=0.5, step=0.05)), column(2, sliderInput(\u0026quot;threshold_feedback\u0026quot;, \u0026quot;Feedback Assignment:\u0026quot;, min=0.0, max=0.90, value=0.5, step=0.05)), column(1, radioButtons(\u0026quot;type\u0026quot;, \u0026quot;Alignment:\u0026quot;, choices = c(\u0026quot;FA\u0026quot;, \u0026quot;Manual\u0026quot;))), column(1, radioButtons(\u0026quot;dx\u0026quot;, \u0026quot;Diagnosis:\u0026quot;, choices = c(\u0026quot;TD\u0026quot;, \u0026quot;SD\u0026quot;, \u0026quot;TD+SD\u0026quot;), selected = \u0026quot;TD+SD\u0026quot;))), fluidRow( column(6, column(8, h3(\u0026quot;Overall Performance:\u0026quot;, style = \u0026quot;font-weight: 800\u0026quot;), span(textOutput(\u0026quot;mp_accuracy\u0026quot;), style = \u0026quot;font-size: 16px\u0026quot;), span(textOutput(\u0026quot;mp_accuracy_phonemes\u0026quot;), style = \u0026quot;font-size: 16px\u0026quot;), span(textOutput(\u0026quot;pf_accuracy\u0026quot;), style = \u0026quot;font-size: 16px\u0026quot;), span(textOutput(\u0026quot;pf_accuracy_phonemes\u0026quot;), style = \u0026quot;font-size: 16px\u0026quot;), h2(\u0026quot; \u0026quot;)), column(4, br(), plotOutput(\u0026quot;phn_conf_matrix\u0026quot;, height=\u0026quot;150px\u0026quot;), h2(\u0026quot; \u0026quot;)), br(), br(), plotOutput(\u0026quot;pf_conf_matrix\u0026quot;, height=\u0026quot;500px\u0026quot;)), column(6, column(6, gt_output(\u0026quot;top_substitutions_table\u0026quot;), h2(\u0026quot; \u0026quot;)), column(6, gt_output(\u0026quot;worst_top_substitutions_table\u0026quot;), h2(\u0026quot; \u0026quot;)), plotOutput(\u0026quot;feature_accuracy\u0026quot;)), ) ) # Define server logic server \u0026lt;- function(input, output) { output$mp_accuracy \u0026lt;- renderText({ mp_data \u0026lt;- current_data() %\u0026gt;% filter(predicted_score_conf \u0026gt;= input$threshold_binary) %\u0026gt;% mutate(binary_accuracy = if_else(actual_score == predicted_score, 1, 0)) mp_data_incorrect \u0026lt;- mp_data %\u0026gt;% filter(actual_score == 0) mp_data_correct \u0026lt;- mp_data %\u0026gt;% filter(actual_score == 1) mp_correct_0 \u0026lt;- sum(mp_data_incorrect$binary_accuracy) mp_total_0 \u0026lt;- nrow(mp_data_incorrect) mp_correct_1 \u0026lt;- sum(mp_data_correct$binary_accuracy) mp_total_1 \u0026lt;- nrow(mp_data_correct) mp_accuracy_0 \u0026lt;- round(mp_correct_0/mp_total_0,2)*100 mp_accuracy_1 \u0026lt;- round(mp_correct_1/mp_total_1,2)*100 mp_balanced_acc \u0026lt;- round((mp_accuracy_0 + mp_accuracy_1)/2, 2) paste(\u0026quot;Mispronunciation detection (balanced) accuracy: \u0026quot;, mp_balanced_acc, \u0026quot;%\u0026quot;, sep=\u0026quot;\u0026quot;) }) output$mp_accuracy_phonemes \u0026lt;- renderText({ mp_data \u0026lt;- current_data() %\u0026gt;% filter(predicted_score_conf \u0026gt;= input$threshold_binary) %\u0026gt;% mutate(binary_accuracy = if_else(actual_score == predicted_score, 1, 0)) mp_total \u0026lt;- nrow(mp_data) mp_overall \u0026lt;- nrow(current_data()) paste(\u0026quot;Phonemes assigned scores: \u0026quot;, mp_total, \u0026quot;/\u0026quot;, mp_overall, sep=\u0026quot;\u0026quot;) }) output$pf_accuracy \u0026lt;- renderText({ pf_data \u0026lt;- current_data() %\u0026gt;% filter(predicted_score_conf \u0026gt;= input$threshold_binary) %\u0026gt;% filter(actual_phn != \u0026#39;-\u0026#39;, predicted_top_error_conf \u0026gt;= input$threshold_feedback, actual_score == 0 \u0026amp; predicted_score == 0) %\u0026gt;% mutate(feedback_accuracy = if_else(predicted_top_error_validity == \u0026quot;True\u0026quot;, 1, 0)) pf_correct \u0026lt;- sum(pf_data$feedback_accuracy) pf_total \u0026lt;- nrow(pf_data) paste(\u0026quot;Feedback accuracy: \u0026quot;, round(pf_correct/pf_total,2)*100, \u0026quot;%\u0026quot;, sep=\u0026quot;\u0026quot;) }) output$pf_accuracy_phonemes \u0026lt;- renderText({ pf_data \u0026lt;- current_data() %\u0026gt;% filter(predicted_score_conf \u0026gt;= input$threshold_binary) %\u0026gt;% filter(actual_phn != \u0026#39;-\u0026#39;, predicted_top_error_conf \u0026gt;= input$threshold_feedback, actual_score == 0 \u0026amp; predicted_score == 0) %\u0026gt;% mutate(feedback_accuracy = if_else(predicted_top_error_validity == \u0026quot;True\u0026quot;, 1, 0)) pf_data_all \u0026lt;- current_data() %\u0026gt;% filter(predicted_score_conf \u0026gt;= input$threshold_binary) %\u0026gt;% filter(actual_phn != \u0026#39;-\u0026#39;, actual_score == 0 \u0026amp; predicted_score == 0) pf_total \u0026lt;- nrow(pf_data) pf_overall \u0026lt;- nrow(pf_data_all) paste(\u0026quot;Mispronounced phonemes assigned feedback: \u0026quot;, pf_total, \u0026quot;/\u0026quot;, pf_overall, sep=\u0026quot;\u0026quot;) }) output$feature_accuracy \u0026lt;- renderPlot({ # Dataframe including phonemes for which feedback was assigned, given # *binary* and *feedback* thresholds. pf_data \u0026lt;- current_data() %\u0026gt;% filter(predicted_score_conf \u0026gt;= input$threshold_binary) %\u0026gt;% filter(actual_phn != \u0026#39;-\u0026#39;, predicted_top_error_conf \u0026gt;= input$threshold_feedback, actual_score == 0 \u0026amp; predicted_score == 0) %\u0026gt;% mutate(feedback_accuracy = if_else(predicted_top_error_validity == \u0026quot;True\u0026quot;, 1, 0)) %\u0026gt;% group_by(predicted_top_error) %\u0026gt;% summarise(fb_acc = sum(feedback_accuracy), fb_total = n()) %\u0026gt;% mutate(percent_correct = fb_acc/fb_total * 100) %\u0026gt;% arrange(desc(fb_total)) %\u0026gt;% ungroup() %\u0026gt;% mutate(predicted_top_error = as.character(predicted_top_error)) x_labs_feature = pf_data$predicted_top_error x_labs_count = c(as.character(pf_data$fb_total)) x_labs \u0026lt;- to_list(for(index in seq(from = 1, to = length(x_labs_count))) paste(x_labs_feature[index], \u0026#39;\\n\u0026#39;, x_labs_count[index], sep = \u0026quot;\u0026quot;)) pf_plot \u0026lt;- pf_data %\u0026gt;% ggplot(aes(x = predicted_top_error, y = percent_correct)) + geom_col(color=rgb(66, 139, 202, max=255), fill=rgb(66, 139, 202, max=255)) + theme_minimal() + xlab(\u0026quot;\\nPredicted Top Phonological Error and # Occurrences\u0026quot;) + ylab(\u0026quot;Percent Correct\u0026quot;) + ggtitle(\u0026quot;Feature-level Accuracy of Assigned Phonological Feedback, by Frequency\\n\u0026quot;) + theme(plot.title = element_text(size = 16, face = \u0026quot;bold\u0026quot;, hjust=0.5), axis.title = element_text(size = 14), axis.text = element_text(size = 11)) + scale_x_discrete(labels=x_labs) pf_plot }) output$pf_conf_matrix \u0026lt;- renderPlot({ data \u0026lt;- current_data() %\u0026gt;% filter(predicted_score_conf \u0026gt;= input$threshold_binary) conf_matrix \u0026lt;- conf_mat(data, actual_top_error, predicted_top_error, dnn=c(\u0026quot;Prediction\u0026quot;,\u0026quot;Truth\u0026quot;)) autoplot(conf_matrix, type=\u0026quot;heatmap\u0026quot;) + scale_y_discrete(position = \u0026quot;left\u0026quot;) + theme_minimal() + scale_fill_gradient(low = \u0026quot;white\u0026quot;, high = rgb(109,159,212, max=255)) + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = \u0026quot;right\u0026quot;, plot.title = element_text(size = 16, face = \u0026quot;bold\u0026quot;, hjust=0.5), axis.title = element_text(size = 14), axis.text = element_text(size = 11)) + ggtitle(\u0026quot;\\nTop Phonological Feature Error Confusion Matrix\\n\u0026quot;) }) output$phn_conf_matrix \u0026lt;- renderPlot({ data \u0026lt;- current_data() %\u0026gt;% mutate(actual_score = as.factor(actual_score), predicted_score = as.factor(predicted_score)) data \u0026lt;- data %\u0026gt;% filter(predicted_score_conf \u0026gt;= input$threshold_binary) conf_matrix \u0026lt;- conf_mat(data, actual_score, predicted_score) autoplot(conf_matrix, type=\u0026quot;heatmap\u0026quot;) + scale_y_discrete(position = \u0026quot;left\u0026quot;) + theme_minimal() + scale_fill_gradient(low = \u0026quot;white\u0026quot;, #high = rgb(66, 139, 202, max=255)) + high = rgb(109,159,212, max=255)) + theme(axis.text.x = element_text(angle = 45, hjust = 0.5), title = element_text(hjust=0.5), legend.position = \u0026quot;right\u0026quot;) + ggtitle(\u0026quot;Binary Classification\\nConfusion Matrix\u0026quot;) }) output$top_substitutions_table \u0026lt;- render_gt({ # Filter to actual substitutions, in total dataset. subs_data_raw \u0026lt;- current_data() %\u0026gt;% filter(predicted_score_conf \u0026gt;= input$threshold_binary, actual_phn != \u0026#39;-\u0026#39;) %\u0026gt;% filter(actual_phn != expected_phn, actual_score == 0) %\u0026gt;% group_by(actual_phn, expected_phn) %\u0026gt;% summarise(total_subs = n(), num_correctly_identified = n() - sum(predicted_score), percent_correctly_identified = (num_correctly_identified/n())) %\u0026gt;% arrange(desc(total_subs)) subs_data_raw %\u0026gt;% head(n=5) %\u0026gt;% ungroup() %\u0026gt;% gt() %\u0026gt;% tab_header(title = md(\u0026quot;**Top Overall Substitutions:**\u0026quot;)) %\u0026gt;% fmt_percent(columns = vars(percent_correctly_identified), decimals = 2) %\u0026gt;% cols_label(actual_phn=\u0026quot;Actual\\nPhoneme\u0026quot;, expected_phn=\u0026quot;Expected\\nPhoneme\u0026quot;, total_subs=\u0026quot;Count\u0026quot;, num_correctly_identified=\u0026quot;Correctly\\nIdentified\\n(#)\u0026quot;, percent_correctly_identified=\u0026quot;Correctly\\nIdentified\\n(%)\u0026quot;) %\u0026gt;% tab_options(heading.title.font.weight = \u0026quot;bolder\u0026quot;, heading.align = \u0026quot;left\u0026quot;, table.font.size = 13, table.border.top.color = \u0026quot;white\u0026quot;) }) output$worst_top_substitutions_table \u0026lt;- render_gt({ # Filter to actual substitutions, in total dataset. subs_data_raw \u0026lt;- current_data() %\u0026gt;% filter(predicted_score_conf \u0026gt;= input$threshold_binary, actual_phn != \u0026#39;-\u0026#39;) %\u0026gt;% filter(actual_phn != expected_phn, actual_score == 0) %\u0026gt;% group_by(actual_phn, expected_phn) %\u0026gt;% summarise(total_subs = n(), num_correctly_identified = n() - sum(predicted_score), percent_correctly_identified = (num_correctly_identified/n())) %\u0026gt;% arrange(desc(total_subs)) %\u0026gt;% filter(percent_correctly_identified \u0026lt; 0.5) subs_data_raw %\u0026gt;% head(n=5) %\u0026gt;% ungroup() %\u0026gt;% gt() %\u0026gt;% tab_header(title = md(\u0026quot;**Top Poorly-Performing Substitutions:**\u0026quot;)) %\u0026gt;% fmt_percent(columns = vars(percent_correctly_identified), decimals = 2) %\u0026gt;% cols_label(actual_phn=\u0026quot;Actual\\nPhoneme\u0026quot;, expected_phn=\u0026quot;Expected\\nPhoneme\u0026quot;, total_subs=\u0026quot;Count\u0026quot;, num_correctly_identified=\u0026quot;Correctly\\nIdentified\\n(#)\u0026quot;, percent_correctly_identified=\u0026quot;Correctly\\nIdentified\\n(%)\u0026quot;) %\u0026gt;% tab_options(heading.title.font.weight = \u0026quot;bolder\u0026quot;, heading.align = \u0026quot;left\u0026quot;, table.font.size = 13, table.border.top.color = \u0026quot;white\u0026quot;) }) current_data \u0026lt;- reactive({ data \u0026lt;- if(input$type == \u0026quot;FA\u0026quot;) data_child_fa else data_child_manual data \u0026lt;- if(input$dx == \u0026quot;TD\u0026quot;) data %\u0026gt;% filter(dx == \u0026quot;TD\u0026quot;) else data data \u0026lt;- if(input$dx == \u0026quot;SD\u0026quot;) data %\u0026gt;% filter(dx == \u0026quot;SD\u0026quot;) else data }) } # Run the application shinyApp(ui = ui, server = server) And that concludes the discussion of this visualization! I hope you enjoyed learning about the process, as well as the data used and the project from which it came, I certainly had fun building the application and discussing the process.\n ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591574400,"objectID":"8af375602e5aabbb0e74140f5a75d4d8","permalink":"https://vigilant-golick-64a677.netlify.app/post/final-viz/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/post/final-viz/","section":"post","summary":"A preliminary note: If the visualization above does not render well on your machine, please visit the application directly, at https://amie-roten.shinyapps.io/Data_Exploration_Phonological_Features/ !\nOverview The interactive visualization above was created primarily as a final project submission for OHSU’s Data Visualization course, but also as a tool to guide investigation of data from a research project I’ve been working on during my time at OHSU.","tags":null,"title":"Comprehensive Exploratory Data Analysis using Shiny!","type":"post"},{"authors":null,"categories":null,"content":" This post is an excerpt from a data visualization lab in which we experimented with use of colors in plots, both with positive and not-so-positive outcomes. In this post, I explore some good, bad, and greyscale color options, and briefly discuss the merits or faults of each. So, lets dive in!\nI know, I know, we’ve probably all seen enough coronavirus visualizations to last a lifetime, but here is one more! I was curious if the great toilet paper panic was a particularly American phenomenon, or if this was an anxiety that struck in the time of coronavirus regardless of nationality. To take a peek into this question, I used Google Trends (https://trends.google.com/) to gather data on search interest for coronavirus and toilet paper across three countries: China, Italy, and the United States, over the past 90 days. I figured this would make for an interesting color choice challenge, since we have three country variables, but each country has information on two different items, so we need to make a two-dimensional distinction.\n# Getting data. Note, the data came in as three separate .csv\u0026#39;s, # I did some preliminary coalescing outside of R to get one single dataset. tp_data \u0026lt;- read_csv(\u0026#39;data/tp_panic.csv\u0026#39;) # Adjusting the data types from simple character/string types. tp_data \u0026lt;- tp_data %\u0026gt;% mutate(Country = as.factor(Country)) %\u0026gt;% mutate(Day = as.Date(Day, \u0026quot;%m/%d/%y\u0026quot;)) %\u0026gt;% mutate(coronavirus = case_when(coronavirus == \u0026#39;\u0026lt;1\u0026#39; ~ 0.5, coronavirus != \u0026#39;\u0026lt;1\u0026#39; ~ as.numeric(coronavirus))) %\u0026gt;% mutate(toilet_paper = case_when(toilet_paper == \u0026#39;\u0026lt;1\u0026#39; ~ 0.5, toilet_paper != \u0026#39;\u0026lt;1\u0026#39; ~ as.numeric(toilet_paper))) glimpse(tp_data) ## Rows: 270 ## Columns: 4 ## $ Day \u0026lt;date\u0026gt; 2020-01-17, 2020-01-18, 2020-01-19, 2020-01-20, 2020-01… ## $ Country \u0026lt;fct\u0026gt; Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, … ## $ toilet_paper \u0026lt;dbl\u0026gt; 0.5, 0.5, 0.5, 0.5, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0, 0.0, 0… ## $ coronavirus \u0026lt;dbl\u0026gt; 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 2.0, 2.0, 3.0, 4.0, 4.0, 8… Version with good color Now, for the plotting, starting with a good color scheme!\nlibrary(cowplot) library(ggpubr) tp_plot_corona \u0026lt;- tp_data %\u0026gt;% ggplot(aes(Day, coronavirus, color = Country)) + geom_line() + scale_color_manual(values = c(\u0026#39;#FFC107\u0026#39;, \u0026#39;#C50049\u0026#39;, \u0026#39;#4897DC\u0026#39;)) + ylab(\u0026quot;Coronavirus\\nInterest\u0026quot;) + xlab(\u0026quot;\u0026quot;) + labs(title = \u0026quot;\u0026#39;Coronavirus\u0026#39; and \u0026#39;Toilet Paper\u0026#39;\\nGoogle Search Interest Levels in Early 2020\u0026quot;) + theme_minimal() tp_plot_tp \u0026lt;- tp_data %\u0026gt;% ggplot(aes(Day, toilet_paper, color = Country)) + geom_line(alpha = 0.5) + scale_color_manual(values = c(\u0026#39;#FFC107\u0026#39;, \u0026#39;#C50049\u0026#39;, \u0026#39;#4897DC\u0026#39;)) + ylab(\u0026quot;Toilet Paper\\nInterest\u0026quot;) + xlab(\u0026quot;Date\u0026quot;) + theme_minimal() tp_plot_full \u0026lt;- ggarrange(tp_plot_corona, tp_plot_tp, heights = c(2, 0.8), ncol = 1, nrow = 2, common.legend = TRUE, align = \u0026#39;v\u0026#39;, legend = \u0026quot;right\u0026quot;) tp_plot_full The function below will check to see how these color selections will be percieved by folks with various types of colorblindness:\n# Seems to work ok for CVD! cvd_grid(tp_plot_full) Alright, that took way longer than it should have! This dataset ended up being really tricky to work with, but I think it was a good exercise in both how to use color to improve interpretation of a plot, but also how color can make a plot much more confusing (as will be demonstrated in the third plot). Although the plot is relatively simple, just six plotted lines, as mentioned above the relationships between the data/lines is a bit more complex than just six discrete/qualitative variables. Therefore, selecting six discrete colors would not make sense, as it would likely obfuscate the fact that each country had two sets of information/two lines, one for their interest/search frequency in/of coronavirus, and one for toilet paper. Connecting the two sets by country by means of color felt like a successful choice, because you can easily look at the two sets of lines and realize that the blue lines (indicating data belonging to the U.S.) show peaks occuring around the same time frame, which was the whole point of the plot. I opted to make the lines for the two different opacities, in order to still make it clear that they were indeed measuring two different things, despite being connected, although this felt like a secondary point, since I opted to create a subplot for the toilet paper data. I didn’t just choose that to offset the toilet paper data, however, I do think that the two different sets of dependent variables could have shared a common y-axis and been distinguished from each other using color, however, since the values for toilet paper interest were so much lower than for coronavirus, and the point was not to compare absolute values of one search term to the other (instead to see if they peaked at the same time), creating subplots in order to emphazise the peaks in the T.P. data made more sense to me. So, my overall take away from the visualization is that, yes, it does look like there was some coronavirus-related toilet paper panic in the United States, but not so much for China or Italy. Priorities!\nFull disclosure, I don’t love the colors used in the plot above. I think they’re starting to veer into ugly territory, a little too saturated for my tastes. But, this blue/yellow/magenta palette seems to work best for colorblind viewers. I’ll need to keep playing around to find a color palette that I like that still works well for those with colorblindness.\n Version with greyscale tp_plot_corona \u0026lt;- tp_data %\u0026gt;% ggplot(aes(Day, coronavirus, color = Country)) + geom_line(linetype=\u0026quot;longdash\u0026quot;) + scale_color_manual(values = c(\u0026#39;#000000\u0026#39;, \u0026#39;#808080\u0026#39;, \u0026#39;#D3D3D3\u0026#39;)) + ylab(\u0026quot;Coronavirus\\nInterest\u0026quot;) + xlab(\u0026quot;\u0026quot;) + labs(title = \u0026quot;\u0026#39;Coronavirus\u0026#39; and \u0026#39;Toilet Paper\u0026#39;\\nGoogle Search Interest Levels in Early 2020\u0026quot;) + theme_minimal() tp_plot_tp \u0026lt;- tp_data %\u0026gt;% ggplot(aes(Day, toilet_paper, color = Country)) + geom_line(alpha = 1) + scale_color_manual(values = c(\u0026#39;#000000\u0026#39;, \u0026#39;#808080\u0026#39;, \u0026#39;#D3D3D3\u0026#39;)) + ylab(\u0026quot;Toilet Paper\\nInterest\u0026quot;) + xlab(\u0026quot;Date\u0026quot;) + theme_minimal() tp_plot_full \u0026lt;- ggarrange(tp_plot_corona, tp_plot_tp, heights = c(2, 0.8), ncol = 1, nrow = 2, common.legend = TRUE, align = \u0026#39;v\u0026#39;, legend = \u0026quot;right\u0026quot;) tp_plot_full Hmm, I’m not as psyched about this one as I was the colored version. Since there are only three “hues” in the original, to distinguish the countries, it was pretty straightforward to translate those into greyscale in a way that ensures they are distinguishable (although I suppose the lightest value is a bit light, I think it is acceptable at least on a screen…I’d have to do some print tests to make sure it was ok in other mediums). Increasing the alpha for the T.P. subplot no longer worked well to set it off, since that made the greys less distinguishable…the increased overlap in the data also didn’t help. I still wanted an easy way for the eye to differentiate the lines in the top plot versus the bottom, and although, in my opinion, complexity in a plot works best when it increases top-to-bottom, making the lines in the bottom plot dashed was just as difficult to visually parse as when they were made more transparent…so I opted to make the top plot lines dashed. I think the greyscale setup works, but it’s not terribly groundbreaking or beautiful.\n Version with dreadful color tp_long \u0026lt;- tp_data %\u0026gt;%pivot_longer(c(coronavirus, toilet_paper), names_to = \u0026quot;searchterms\u0026quot;, values_to = \u0026quot;interest\u0026quot;) %\u0026gt;% mutate(Country_SearchTerm = paste(Country, searchterms, sep = \u0026quot;: \u0026quot;), Country = NULL, searchterms = NULL) tp_plot_bad \u0026lt;- tp_long %\u0026gt;% ggplot(aes(Day, interest, color = Country_SearchTerm)) + geom_line() + scale_color_manual(values = c(\u0026#39;#800000\u0026#39;, \u0026#39;#FF9999\u0026#39;, \u0026#39;#000075\u0026#39;, \u0026#39;#6666DB\u0026#39;, \u0026#39;#469990\u0026#39;, \u0026#39;#ACFFF6\u0026#39;)) + ylab(\u0026quot;Search Interest\u0026quot;) + xlab(\u0026quot;\u0026quot;) + labs(title = \u0026quot;\u0026#39;Coronavirus\u0026#39; and \u0026#39;Toilet Paper\u0026#39;\\nGoogle Search Interest Levels in Early 2020\u0026quot;) + theme_minimal() tp_plot_bad Above was my original attempt at this plot…I had good intentions – my goal was still wanted to link the data together by country, but I ended up separating the coronavirus data from the T.P. data by lightening the colors 40% (according to https://www.hexcolortool.com/#479a90). Wow, this resulted in some ugly colors, and really difficult to see not only due to color, but also due to the relative values/heights of the datasets. Additionally, simply lightening the colors does not guarantee that the darker and lighter versions will feel intuitively cohesive, in fact, Italy and the U.S.’s colors begin to look like a spectrum when they are broken down into a light and dark version each (this can be observed best in the legend)…which is definitely not something that makes sense with this data. Also, this would not work well for a colorblind viewer…overall, this is a much less effective plot than the original, in my opinion!\nThere we have it, a short exploration of color choice in at least one type of line plot. Happy plotting!\n ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591574400,"objectID":"7e7c021ce698abf4b48b590b4895737a","permalink":"https://vigilant-golick-64a677.netlify.app/post/tp-panic/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/post/tp-panic/","section":"post","summary":"This post is an excerpt from a data visualization lab in which we experimented with use of colors in plots, both with positive and not-so-positive outcomes. In this post, I explore some good, bad, and greyscale color options, and briefly discuss the merits or faults of each.","tags":null,"title":"Exploring Color with Coronavirus and Toilet Paper Data!","type":"post"},{"authors":null,"categories":null,"content":" For this post, I want to showcase my first interactive Shiny app, both the final product, and below it, the code used to generate the application. Although yes, this does use more COVID data, it was a very nice exploration of how to go about using Shiny, and I must say, after working with it, I think it’s a tool that I will start using often to explore large datasets!\nThe Application:  This application uses Kieran Healy’s covdata package, a comprehensive collection of data relating to the Coronavirus pandemic. I decided to use the Apple U.S. Mobility dataset, specifically the vehicular-travel data, to create the application above. The app includes three identical widgets that can be used to select three different states in order to compare and contrast the data. The widgets’ upper plots show the daily case increase, and the bottom plot shows the daily mobility index, that is how the day’s mobility data differs from the baseline taken in January. The yellow indicates a lower level of travel for that particular day, and the green indicates more travel. Please explore the application and see what interesting observations you can make!\n The Code: Below is the code used to generate the Shiny app\nThe first section is simply the data wrangling process, processing the raw data in the covdata package in order to get it in the correct format for use in the plots. This is not an inherant piece of creating a Shiny application, but it useful to demonstrate the process of reshaping and winnowing down the dataset for this particular use!\nlibrary(shiny) library(gt) library(covdata) library(ggpubr) library(tidyverse) case_data \u0026lt;- nytcovstate locations \u0026lt;- sort(unique(case_data$state)) mobility_data \u0026lt;- apple_mobility %\u0026gt;% filter(region %in% locations) %\u0026gt;% select(-alternative_name, -geo_type) %\u0026gt;% mutate(state = region, region = NULL) data \u0026lt;- full_join(mobility_data, case_data) %\u0026gt;% mutate(fips = replace_na(fips, 0), cases = replace_na(cases, 0), deaths = replace_na(deaths, 0)) %\u0026gt;% group_by(state) %\u0026gt;% arrange(date, .by_group = TRUE) %\u0026gt;% mutate(daily_cases = (cases - (lag(cases)))) %\u0026gt;% ungroup() %\u0026gt;% mutate(daily_cases = replace_na(daily_cases, 0)) transpo \u0026lt;- c(unique(data$transportation_type)) vec_brks \u0026lt;- c(-50, 0, 50) vec_labs \u0026lt;- vec_brks + 100 start_date \u0026lt;- sort(data$date)[1] end_date \u0026lt;- sort(desc(data$date))[1] Creating a Shiny application requires two main components, the ui componant, which is where the application creator can both tweak the layout of the application, as well as create interactive elements where the user can select from a number of options. For example, dateRangeInput() creates a widget where the user can enter a range of dates. The creator can select what the possible range can be, eliminate possibility for invalid date selections. There are many different input functions, and the elements can be arranged in many configurations in order to get the app looking just right! It’s a wonderland (and/or a real time-sink) for people like me who get endless satisfaction from minor nudges and adjustments to get things looking just right. :D\nui \u0026lt;- fluidPage( titlePanel(\u0026quot;Apple U.S. Mobility Dataset during COVID-19 Crisis\u0026quot;), fluidRow( column(5, h3(\u0026quot;Please begin by selecting your desired date range: \u0026quot;)), column(3, dateRangeInput(\u0026#39;dateRange\u0026#39;, label = \u0026#39;\u0026#39;, format=\u0026#39;mm-dd-yyyy\u0026#39;, separator=\u0026#39; to \u0026#39;, start = \u0026#39;2020-01-13\u0026#39;, end = \u0026#39;2020-05-13\u0026#39;, min = \u0026#39;2020-01-13\u0026#39;, max = \u0026#39;2020-05-13\u0026#39;)), column(4, h5(\u0026quot;Upper plot displays the daily increase in COVID-19 cases for selected location. Lower plot shows corresponding Apple Maps\u0026#39; mobility data, in terms of trends in driving since mid-January 2020 (dark green indicates a relative increase, yellow indicates a relative decrease). Did less driving correspond to \\\u0026quot;flattening the curve\\\u0026quot;? We can find out!\u0026quot;)) ), hr(), fluidRow( column(4, plotOutput(\u0026quot;plot1\u0026quot;)), column(4, plotOutput(\u0026quot;plot2\u0026quot;)), column(4, plotOutput(\u0026quot;plot3\u0026quot;)) ), fluidRow( column(3, offset = 1, selectInput(\u0026quot;loc1\u0026quot;, label=\u0026quot;Location 1\u0026quot;, multiple=FALSE, locations, width=\u0026#39;70%\u0026#39;, size=5, selectize=FALSE)), column(3, offset = 1, selectInput(\u0026quot;loc2\u0026quot;, label=\u0026quot;Location 2\u0026quot;, multiple=FALSE, locations, width=\u0026#39;70%\u0026#39;, size=5, selectize=FALSE)), column(3, offset = 1, selectInput(\u0026quot;loc3\u0026quot;, label=\u0026quot;Location 3\u0026quot;, multiple=FALSE, locations, width=\u0026#39;70%\u0026#39;, size=5, selectize=FALSE)), ), h5(\u0026quot;Data from Kieran Healy\u0026#39;s covdata package: https://kjhealy.github.io/covdata/. Bottom plots *very heavily* inspired by apple_mobility dataset vignette!\u0026quot;) ) The second main component is the server section. This is where the dynamic outputs such as plots, text, and other elements, are created and updated, and then can be placed via the ui section. In this case, we have six plots (three sets of two) which use input from the selection widgets in the section above to dynamically change and reflect the subset of the data the user would like to see.\nserver \u0026lt;- function(input, output, session) { output$plot1 \u0026lt;- renderPlot({ case_plot1 \u0026lt;- data %\u0026gt;% filter(state == input$loc1, #transportation_type %in% transpo_filter(), date \u0026gt;= input$dateRange[1] \u0026amp; date \u0026lt;= input$dateRange[2]) %\u0026gt;% ggplot(mapping=aes(x=date, y=daily_cases)) + geom_line() + scale_y_log10(limits=c(1, 3000)) + theme(legend.position = \u0026quot;none\u0026quot;) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;Daily Case\\nIncrease\u0026quot;) mobi_plot1 \u0026lt;- data %\u0026gt;% filter(state == input$loc1, #transportation_type %in% transpo_filter(), date \u0026gt;= input$dateRange[1] \u0026amp; date \u0026lt;= input$dateRange[2]) %\u0026gt;% mutate(over_under = index \u0026lt; 100, index = index - 100) %\u0026gt;% ggplot(mapping=aes(x=date, y=index, fill=over_under, col=over_under)) + geom_col() + scale_color_manual(values = c(\u0026#39;#004D40\u0026#39;, \u0026#39;#FFC107\u0026#39;)) + scale_fill_manual(values = c(\u0026#39;#004D40\u0026#39;, \u0026#39;#FFC107\u0026#39;)) + theme(legend.position = \u0026quot;none\u0026quot;) + scale_y_continuous(limits = c(-150, 150)) + geom_hline(yintercept = 0, color = \u0026quot;gray40\u0026quot;) + xlab(\u0026quot;Date\u0026quot;) + ylab(\u0026quot;Mobility Index\u0026quot;) ggarrange(case_plot1, mobi_plot1, heights = c(0.8, 2), ncol = 1, nrow = 2, align = \u0026#39;v\u0026#39;, legend = NULL) }) output$plot2 \u0026lt;- renderPlot({ case_plot2 \u0026lt;- data %\u0026gt;% filter(state == input$loc2, #transportation_type %in% transpo_filter(), date \u0026gt;= input$dateRange[1] \u0026amp; date \u0026lt;= input$dateRange[2]) %\u0026gt;% ggplot(mapping=aes(x=date, y=daily_cases)) + geom_line() + scale_y_log10(limits=c(1, 3000)) + theme(legend.position = \u0026quot;none\u0026quot;) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;Daily Case\\nIncrease\u0026quot;) mobi_plot2 \u0026lt;- data %\u0026gt;% filter(state == input$loc2, #transportation_type %in% transpo_filter(), date \u0026gt;= input$dateRange[1] \u0026amp; date \u0026lt;= input$dateRange[2]) %\u0026gt;% mutate(over_under = index \u0026lt; 100, index = index - 100) %\u0026gt;% ggplot(mapping=aes(x=date, y=index, fill=over_under, col=over_under)) + geom_col() + theme(legend.position = \u0026quot;none\u0026quot;) + scale_y_continuous(limits = c(-150, 150)) + scale_color_manual(values = c(\u0026#39;#004D40\u0026#39;, \u0026#39;#FFC107\u0026#39;)) + scale_fill_manual(values = c(\u0026#39;#004D40\u0026#39;, \u0026#39;#FFC107\u0026#39;)) + geom_hline(yintercept = 0, color = \u0026quot;gray40\u0026quot;) + xlab(\u0026quot;Date\u0026quot;) + ylab(\u0026quot;Mobility Index\u0026quot;) ggarrange(case_plot2, mobi_plot2, heights = c(0.8, 2), ncol = 1, nrow = 2, align = \u0026#39;v\u0026#39;, legend = NULL) }) output$plot3 \u0026lt;- renderPlot({ case_plot3 \u0026lt;- data %\u0026gt;% filter(state == input$loc3, #transportation_type %in% transpo_filter(), date \u0026gt;= input$dateRange[1] \u0026amp; date \u0026lt;= input$dateRange[2]) %\u0026gt;% ggplot(mapping=aes(x=date, y=daily_cases)) + theme(legend.position = \u0026quot;none\u0026quot;) + scale_y_log10(limits=c(1, 3000)) + geom_line() + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;Daily Case\\nIncrease\u0026quot;) mobi_plot3 \u0026lt;- data %\u0026gt;% filter(state == input$loc3, #transportation_type %in% transpo_filter(), date \u0026gt;= input$dateRange[1] \u0026amp; date \u0026lt;= input$dateRange[2]) %\u0026gt;% mutate(over_under = index \u0026lt; 100, index = index - 100) %\u0026gt;% ggplot(mapping=aes(x=date, y=index, fill=over_under, col=over_under)) + geom_col() + theme(legend.position = \u0026quot;none\u0026quot;) + scale_color_manual(values = c(\u0026#39;#004D40\u0026#39;, \u0026#39;#FFC107\u0026#39;)) + scale_fill_manual(values = c(\u0026#39;#004D40\u0026#39;, \u0026#39;#FFC107\u0026#39;)) + scale_y_continuous(limits = c(-150, 150)) + geom_hline(yintercept = 0, color = \u0026quot;gray40\u0026quot;) + xlab(\u0026quot;Date\u0026quot;) + ylab(\u0026quot;Mobility Index\u0026quot;) ggarrange(case_plot3, mobi_plot3, heights = c(0.8, 2), ncol = 1, nrow = 2, align = \u0026#39;v\u0026#39;, legend = NULL) }) } shinyApp(ui = ui, server = server) I did not go into much depth explaining the ins-and-outs of creating a Shiny app, but hopefully this gives a quick taste of what can be done with the library, and inspires you to make a Shiny app of your own! More information on how to use Shiny can be found here: https://shiny.rstudio.com/tutorial/, and in many places on the web.\n ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591574400,"objectID":"9376db7896385b6e416fd1209fd73c76","permalink":"https://vigilant-golick-64a677.netlify.app/post/covid-shiny/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/post/covid-shiny/","section":"post","summary":"For this post, I want to showcase my first interactive Shiny app, both the final product, and below it, the code used to generate the application. Although yes, this does use more COVID data, it was a very nice exploration of how to go about using Shiny, and I must say, after working with it, I think it’s a tool that I will start using often to explore large datasets!","tags":null,"title":"Exploring COVID-19 Data with Shiny!","type":"post"},{"authors":null,"categories":null,"content":" This post is devoted to answering a couple of questions about flights in and out of PDX, in the form of custom R tables (primarily using the fantastic gt library!). Not only will the questions be answered and tables displayed, but the code for how to wrangle the data and build the tables is included, as well as some narration on what did and did not work. On we go!\nAnalysis #1: Most Traveled Destination, per Month I’d like to take a look at which destinations departing from PDX are the most frequently travelled, broken down by month, along with the flight count for each destination. It’s tricky to make predictions, but I’d think that a destination like Hawaii or Florida might be more popular in the cooler months, whereas more exotic, internations locations like London or Japan might be more popular in the warmer months, corresponding to the summertime where many tend to take longer trips. But, let’s take a look and find out!\npdx_flights_dest \u0026lt;- flights %\u0026gt;% filter(origin == \u0026quot;PDX\u0026quot;) %\u0026gt;% group_by(month, dest) %\u0026gt;% summarise(n = n()) %\u0026gt;% group_by(month) %\u0026gt;% top_n(1, n) pdx_flights_dest ## # A tibble: 12 x 3 ## # Groups: month [12] ## month dest n ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 1 SFO 446 ## 2 2 SFO 389 ## 3 3 SFO 462 ## 4 4 SFO 450 ## 5 5 SFO 464 ## 6 6 SFO 440 ## 7 7 SFO 432 ## 8 8 SFO 437 ## 9 9 SFO 438 ## 10 10 SFO 442 ## 11 11 SFO 391 ## 12 12 SFO 388 Wow…what a terribly dull outcome, not particularly table-worthy. Let’s try excluding San Francisco and see what shakes out. We’ll take a chance and go ahead and print this information in an attractive table format, using the gt library:\npdx_flights_dest_noSF \u0026lt;- flights %\u0026gt;% filter(origin == \u0026quot;PDX\u0026quot;) %\u0026gt;% arrange(month) %\u0026gt;% group_by(month, dest) %\u0026gt;% summarise(n = n()) %\u0026gt;% group_by(month) %\u0026gt;% filter(dest != \u0026quot;SFO\u0026quot;) %\u0026gt;% top_n(1, n) %\u0026gt;% ungroup() %\u0026gt;% mutate(month = month.name[month]) pdx_flights_dest_noSF ## # A tibble: 12 x 3 ## month dest n ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 January DEN 323 ## 2 February DEN 276 ## 3 March LAX 346 ## 4 April LAX 348 ## 5 May LAX 387 ## 6 June DEN 354 ## 7 July DEN 359 ## 8 August DEN 373 ## 9 September DEN 369 ## 10 October DEN 366 ## 11 November DEN 295 ## 12 December PHX 286 pdx_flights_dest_noSF_gt \u0026lt;- pdx_flights_dest_noSF %\u0026gt;% gt() %\u0026gt;% # NOTE: While these options add striping when the # .Rmd file is rendered locally, the striping is not # present when deployed. Oddly, removing this option # *adds* striping to deployed file. The more you know! # tab_options(row.striping.background_color = \u0026quot;#f9f9f9\u0026quot;, # row.striping.include_table_body = TRUE, # row.striping.include_stub = TRUE) %\u0026gt;% tab_header(title=\u0026quot;Top Destinations out of PDX, By Month\u0026quot;, subtitle=\u0026quot;(Excluding SFO)\u0026quot;) %\u0026gt;% cols_label(month = \u0026quot;Month\u0026quot;, dest = \u0026quot;Destination\u0026quot;, n = \u0026quot;Number of Flights\u0026quot;) %\u0026gt;% cols_align(align = \u0026quot;center\u0026quot;, columns = vars(dest)) pdx_flights_dest_noSF_gt html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #hqqbgipsva .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #hqqbgipsva .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hqqbgipsva .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #hqqbgipsva .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #hqqbgipsva .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hqqbgipsva .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hqqbgipsva .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #hqqbgipsva .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #hqqbgipsva .gt_column_spanner_outer:first-child { padding-left: 0; } #hqqbgipsva .gt_column_spanner_outer:last-child { padding-right: 0; } #hqqbgipsva .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #hqqbgipsva .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #hqqbgipsva .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #hqqbgipsva .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #hqqbgipsva .gt_from_md  :first-child { margin-top: 0; } #hqqbgipsva .gt_from_md  :last-child { margin-bottom: 0; } #hqqbgipsva .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #hqqbgipsva .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #hqqbgipsva .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hqqbgipsva .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #hqqbgipsva .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hqqbgipsva .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #hqqbgipsva .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hqqbgipsva .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hqqbgipsva .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #hqqbgipsva .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hqqbgipsva .gt_sourcenote { font-size: 90%; padding: 4px; } #hqqbgipsva .gt_left { text-align: left; } #hqqbgipsva .gt_center { text-align: center; } #hqqbgipsva .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #hqqbgipsva .gt_font_normal { font-weight: normal; } #hqqbgipsva .gt_font_bold { font-weight: bold; } #hqqbgipsva .gt_font_italic { font-style: italic; } #hqqbgipsva .gt_super { font-size: 65%; } #hqqbgipsva .gt_footnote_marks { font-style: italic; font-size: 65%; }   Top Destinations out of PDX, By Month   (Excluding SFO)    Month Destination Number of Flights    January DEN 323   February DEN 276   March LAX 346   April LAX 348   May LAX 387   June DEN 354   July DEN 359   August DEN 373   September DEN 369   October DEN 366   November DEN 295   December PHX 286    I suppose this is somewhat interesting in a couple of respects. First, we can see that, at least judging by the count for the top destination, the airport is busier during the warmer months, with November through February having markedly fewer flights than the other part of the year. We can see that Denver is a popular/common destination year-round (I think PDX-DEN is a big Frontier route, often on my way to visit family Illinois I end up with a layover there). But, during the cooler months, the destinations sway a bit warmer; Los Angeles and Phoenix.\nI chose to use gt since I really like the clean look and simplicity of the tables it generates, especially for a straightforward dataset like this one. I didn’t end up changing the defaults much, since I liked the column/row separations already. I did center the destination column, as it was left-aligned by default, and butted up against the months a bit too much. The destination and flight number columns make clean lines, and it is easy to read them up-and-down, and that really is the most important comparison here; how each column relates to the next sequentially/temporily, not necessarily how the destination relates to its’ associated number of flights.\n Analysis #2: Finding Busy Aircraft I am a nervous flyer, and definitely notice the wear and tear on a plane before I get on it…the longer it’s been in the air, the more reliable it has shown itself to be? So I tell myself…hah! But, I thought this was an interesting line of investigation. I’m going to make a table that displays the top five aircraft with the most flight time accumulated, and also include the associated airline, and the top two routes each aircraft flies.\npdx_flights_aircraft \u0026lt;- flights %\u0026gt;% filter(origin == \u0026quot;PDX\u0026quot;, tailnum!=\u0026quot;\u0026quot;) %\u0026gt;% select(tailnum, dest, air_time, carrier) %\u0026gt;% group_by(tailnum, carrier, dest) %\u0026gt;% summarize(airtime = sum(air_time)) %\u0026gt;% add_count(name = \u0026quot;total_airtime\u0026quot;, wt=airtime) %\u0026gt;% arrange(desc(total_airtime), desc(airtime)) first_set = seq(1, 15, by=3) second_set = seq(2, 15, by=3) third_set = seq(3, 15, by=3) pdx_flights_aircraft_filtered \u0026lt;- pdx_flights_aircraft %\u0026gt;% filter(total_airtime %in% unique(pdx_flights_aircraft$total_airtime)[1:5]) %\u0026gt;% group_by(tailnum) %\u0026gt;% top_n(3, wt = airtime) %\u0026gt;% select(-airtime) %\u0026gt;% mutate(dest_rank = case_when(row_number() %in% first_set ~ \u0026quot;dest_1\u0026quot;, row_number() %in% second_set ~ \u0026quot;dest_2\u0026quot;, row_number() %in% third_set ~ \u0026quot;dest_3\u0026quot;)) %\u0026gt;% pivot_wider(names_from = dest_rank, values_from = dest) %\u0026gt;% mutate(carrier = ifelse(carrier == \u0026#39;OO\u0026#39;, \u0026quot;SkyWest\u0026quot;, \u0026quot;Alaska\u0026quot;)) %\u0026gt;% ungroup() pdx_flights_aircraft_filtered %\u0026gt;% gt() %\u0026gt;% # tab_options(row.striping.background_color = \u0026quot;#f9f9f9\u0026quot;, # row.striping.include_table_body = TRUE, # row.striping.include_stub = TRUE) %\u0026gt;% tab_header(title=\u0026quot;Busiest Aircraft, by Total Airtime\u0026quot;) %\u0026gt;% tab_spanner(label = \u0026quot;Aircraft Details\u0026quot;, columns = vars(tailnum, carrier, total_airtime)) %\u0026gt;% tab_spanner(label = \u0026quot;Busiest Routes\u0026quot;, columns = vars(dest_1, dest_2, dest_3)) %\u0026gt;% cols_label(tailnum = \u0026quot;Tail\\nNumber\u0026quot;, carrier = \u0026quot;Carrier\u0026quot;, total_airtime = \u0026quot;Total Airtime \\n (min)\u0026quot;, dest_1 = \u0026quot;First\u0026quot;, dest_2 = \u0026quot;Second\u0026quot;, dest_3 = \u0026quot;Third\u0026quot;) %\u0026gt;% cols_align(align = \u0026quot;center\u0026quot;, columns = vars(total_airtime, dest_1, dest_2)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #qnwfrylmok .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qnwfrylmok .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qnwfrylmok .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qnwfrylmok .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #qnwfrylmok .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qnwfrylmok .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qnwfrylmok .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qnwfrylmok .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qnwfrylmok .gt_column_spanner_outer:first-child { padding-left: 0; } #qnwfrylmok .gt_column_spanner_outer:last-child { padding-right: 0; } #qnwfrylmok .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #qnwfrylmok .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #qnwfrylmok .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qnwfrylmok .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qnwfrylmok .gt_from_md  :first-child { margin-top: 0; } #qnwfrylmok .gt_from_md  :last-child { margin-bottom: 0; } #qnwfrylmok .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qnwfrylmok .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #qnwfrylmok .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qnwfrylmok .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #qnwfrylmok .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qnwfrylmok .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qnwfrylmok .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qnwfrylmok .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qnwfrylmok .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #qnwfrylmok .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qnwfrylmok .gt_sourcenote { font-size: 90%; padding: 4px; } #qnwfrylmok .gt_left { text-align: left; } #qnwfrylmok .gt_center { text-align: center; } #qnwfrylmok .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qnwfrylmok .gt_font_normal { font-weight: normal; } #qnwfrylmok .gt_font_bold { font-weight: bold; } #qnwfrylmok .gt_font_italic { font-style: italic; } #qnwfrylmok .gt_super { font-size: 65%; } #qnwfrylmok .gt_footnote_marks { font-style: italic; font-size: 65%; }   Busiest Aircraft, by Total Airtime       Aircraft Details  Busiest Routes    Tail Number Carrier Total Airtime (min) First Second Third    N225AG SkyWest 34805 ONT BUR TUS   N219AG SkyWest 31167 BUR TUS SBA   N614AS Alaska 28931 SNA LAS SFO   N611AS Alaska 27697 SNA LAS ORD   N216AG SkyWest 27471 BUR ONT SBA    This was a pretty fun, but involved task. I first played a bit with kable in an attempt to do something different, but it just didn’t make a very attractive table, so I switched back to gt. Since there are a number of different types of information included in this table, I decided to make use of the tab_spanner option to call attention to that fact and do a bit of grouping. Then, I added striping, since in this case, the important grouping is by row, not necessarily comparing each row to each other, so I decided to encourage horizontal scanning of the table using this method.\n Analysis #3 Time of Day I’m curious to see whether certain destinations commonly correspond to morning, afternoon/evening, or red-eye flights, and also the relative representation of each type of flight. I’ll consider flights that leave between 5-11am morning flights, 11-8pm afternoon/evening flights, and the remainder red-eye flights (although these distinctions may not be perfect). I think this table is going to end up looking rather similar to the one above…we will see.\nfirst_set = seq(1, 15, by=5) second_set = seq(2, 15, by=5) third_set = seq(3, 15, by=5) fourth_set = seq(4, 15, by=5) fifth_set = seq(5, 15, by=5) pdx_flights_timeofday \u0026lt;- flights %\u0026gt;% filter(origin == \u0026quot;PDX\u0026quot;, !is.na(dep_time))%\u0026gt;% select(dest, dep_time) %\u0026gt;% mutate(time_of_day = case_when(dep_time \u0026gt;= 500 \u0026amp; dep_time \u0026lt; 1100 ~ \u0026quot;morning\u0026quot;, dep_time \u0026gt;= 1100 \u0026amp; dep_time \u0026lt; 1900 ~ \u0026quot;evening\u0026quot;, TRUE ~ \u0026quot;redeye\u0026quot;)) %\u0026gt;% group_by(time_of_day, dest) %\u0026gt;% count() %\u0026gt;% group_by(time_of_day) %\u0026gt;% arrange(desc(n)) %\u0026gt;% top_n(5, n) %\u0026gt;% mutate(dest_rank = case_when(row_number() %in% first_set ~ \u0026quot;dest_1\u0026quot;, row_number() %in% second_set ~ \u0026quot;dest_2\u0026quot;, row_number() %in% third_set ~ \u0026quot;dest_3\u0026quot;, row_number() %in% fourth_set ~ \u0026quot;dest_4\u0026quot;, row_number() %in% fifth_set ~ \u0026quot;dest_5\u0026quot;)) %\u0026gt;% pivot_wider(names_from = c(time_of_day), values_from = c(dest, n)) %\u0026gt;% mutate(dest_rank = NULL) column_order \u0026lt;- c(\u0026quot;dest_morning\u0026quot;, \u0026quot;n_morning\u0026quot;, \u0026quot;dest_evening\u0026quot;, \u0026quot;n_evening\u0026quot;, \u0026quot;dest_redeye\u0026quot;, \u0026quot;n_redeye\u0026quot;) pdx_flights_timeofday \u0026lt;- pdx_flights_timeofday[, column_order] pdx_flights_timeofday %\u0026gt;% gt() %\u0026gt;% # tab_options(row.striping.background_color = \u0026quot;#f9f9f9\u0026quot;, # row.striping.include_table_body = TRUE, # row.striping.include_stub = TRUE) %\u0026gt;% tab_header(title=\u0026quot;Most Frequent Destinations from PDX\u0026quot;, subtitle = \u0026quot;by Time of Day\u0026quot;) %\u0026gt;% tab_spanner(label = \u0026quot;Morning\u0026quot;, columns = vars(dest_morning, n_morning)) %\u0026gt;% tab_spanner(label = \u0026quot;Afternoon\u0026quot;, columns = vars(dest_evening, n_evening)) %\u0026gt;% tab_spanner(label = \u0026quot;Redeye\u0026quot;, columns = vars(dest_redeye, n_redeye)) %\u0026gt;% cols_label(dest_morning = \u0026quot;Destination\u0026quot;, n_morning = \u0026quot;Flights\u0026quot;, dest_evening = \u0026quot;Destination\u0026quot;, n_evening = \u0026quot;Flights\u0026quot;, dest_redeye = \u0026quot;Destination\u0026quot;, n_redeye = \u0026quot;Flights\u0026quot;) %\u0026gt;% cols_align(align = \u0026quot;center\u0026quot;, columns = vars(dest_morning, dest_evening, dest_redeye)) %\u0026gt;% summary_rows(columns = vars(n_morning, n_evening, n_redeye), fns = list(Total = ~sum(.)), formatter = fmt_number, decimals = 0) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #qlkvcqyudc .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #qlkvcqyudc .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qlkvcqyudc .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #qlkvcqyudc .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #qlkvcqyudc .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qlkvcqyudc .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #qlkvcqyudc .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #qlkvcqyudc .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #qlkvcqyudc .gt_column_spanner_outer:first-child { padding-left: 0; } #qlkvcqyudc .gt_column_spanner_outer:last-child { padding-right: 0; } #qlkvcqyudc .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #qlkvcqyudc .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #qlkvcqyudc .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #qlkvcqyudc .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #qlkvcqyudc .gt_from_md  :first-child { margin-top: 0; } #qlkvcqyudc .gt_from_md  :last-child { margin-bottom: 0; } #qlkvcqyudc .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #qlkvcqyudc .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #qlkvcqyudc .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qlkvcqyudc .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #qlkvcqyudc .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #qlkvcqyudc .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #qlkvcqyudc .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #qlkvcqyudc .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qlkvcqyudc .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #qlkvcqyudc .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #qlkvcqyudc .gt_sourcenote { font-size: 90%; padding: 4px; } #qlkvcqyudc .gt_left { text-align: left; } #qlkvcqyudc .gt_center { text-align: center; } #qlkvcqyudc .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #qlkvcqyudc .gt_font_normal { font-weight: normal; } #qlkvcqyudc .gt_font_bold { font-weight: bold; } #qlkvcqyudc .gt_font_italic { font-style: italic; } #qlkvcqyudc .gt_super { font-size: 65%; } #qlkvcqyudc .gt_footnote_marks { font-style: italic; font-size: 65%; }   Most Frequent Destinations from PDX   by Time of Day     Morning  Afternoon  Redeye    Destination Flights Destination Flights Destination Flights     DEN 1921 SFO 2495 SFO 836    SFO 1780 DEN 1643 PHX 587    PHX 1454 PHX 1504 SJC 572    ORD 1409 SLC 1389 ANC 517    LAX 1316 LAS 1308 LAX 426   Total \u0026mdash; 7,880 \u0026mdash; 8,339 \u0026mdash; 2,938    For this table, as opposed to the table in the second section, I wanted to encourage vertical scanning, to encourage the viewer to note the sequential ordering of the rows, i.e., the first row corresponds to the top flight for each timeframe. Fortunately, I didn’t have to change much from the default gt settings, just tinker with a titles to get the columns arranged in a more harmonious way. I really wanted to find a way to visually separate each timeframe’s data, but had trouble finding how to add vertical bars to separate the groups…although that may have ended up a bit chaotic anyway. Spanning labels will have to do! I do feel like adding the row for the total flights helped a bit, as we can distinctively see three values, indicating three groups. I think this is a pretty cool analysis of the dataset, particularly because it shows rather in-depth information about the distribution of top flights for each time of day, but also the total flights. Redeyes are definitely less popular/frequent, unsurprisingly!\nAs we can see, there are many interesting aspects of the flights dataset that can be explored using nothing more than the humble table. With gt, we can create elegant, easy-to-read tables even for rather complex data!\n ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591574400,"objectID":"b10ad2a66c3e75fb2dbe28e926742bdc","permalink":"https://vigilant-golick-64a677.netlify.app/post/pdx-flights/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/post/pdx-flights/","section":"post","summary":"This post is devoted to answering a couple of questions about flights in and out of PDX, in the form of custom R tables (primarily using the fantastic gt library!","tags":null,"title":"Investigating PDX Flights Data using Tables!","type":"post"},{"authors":null,"categories":null,"content":"A Population Pyramid, also known as an Age and Sex Pyramid is a histogram-like visualization often used to provide a succinct, elegant summary of differences in distribution of groups within a population. The population is broken up by a two-level factor variable, with one on each side of the visualization, and then the data is binned based on a continuous variable. Although population pyramids typically use a factor variable of sex (male and female), and a continuous variable of age, and are often used to convey such information as a country’s fertility rate, the visualization can be generalized to other situations, in which it may be referred to as simply a “dual-sided histogram”. The type of visualization is a handy way to enhance a typical histogram by splitting it into two factor variables in order to facilitate comparison across groups within a population, while still being able to make observations about the population as a whole.\nHow to Read a Population Pyramid Visualization There are several useful takeaways that can be gleaned quickly and easily using a population pyramid:\n  How the overall distribution changes on the basis of age: To get a sense of the age distribution for the overall population, one can simply examine the shape of the plot/pyramid, focusing on how width changes from top to bottom. The wider the plot at any particular height, the more individuals in the age group corresponding to that position on the y-axis contribute to the overall population. So, if the “base”/bottom is wide compared to the rest of the plot, giving it a “pyramid”-like shape, the population is comprised of mostly younger individuals. However, if the overall “pyramid” is more rectangular, the age distribution is balanced. An upside-down pyramid would indicate a generally older population, and a “bulge” could indicate something like a baby boom around that particular age bracket’s birth years.\n  How the overall distribution differs based on sex: This is a fairly simple piece of information to glean from this type of plot – if the pyramid is heavier on one side than the other, then the population is skewed more heavily toward one particular sex than the other.\n  How the sex distribution changes as a function of age: We also can take into account both of the variables above and determine whether the sex distribution differs based on the age of that segment of the population. It’s difficult to think of an example that is not tragic, but, say, a plot of the demographics of the U.S. shortly after World War II may have more females than males in the age bracket 18-25 years old, but would likely be balanced in age brackets younger and older. In this case, the pyramid would be vertically asymmetric, with a “notch” on the male side at that age bracket. Examining the plot for these notches is a quick, simple way to determine whether the sex distribution differs for a particular age group.\n  Finally, in traditional population pyramids which depict age/sex distribution in a particular region, there are three shapes so commonly seen that they are given specific labels: expanding, contracting, and stationary, as shown below (graphic, as well as paraphrased information on each shape from a paper by Daniel Staetsky, full citation can be found in final section):\nA very triangular shape, with many more young representatives than older, as in the expanding pyramid, is indicative of a population that is growing, along with high fertility/mortality rates. The contracting shape, where the pyramid is pinched at the base is indicative of low fertility/mortality rates, corresponding to a population with few young people, relative to the rest of the group. Finally, the stationary pyramid, which has a columnal shape, tends to correspond to stable populations, with balanced, low fertility and mortality rates.\nDisclosure: Given the wide spectrum of gender identities present in modern society, I can absolutely understand how some may object to the binary nature of the population pyramid. Although I, and most who employ this type of visualization, specify the binary factor as biological “sex”, not gender, issues still remain. After a quick search, it doesn’t look like many have addressed this issue currently, but I would not be surprised to see multi-way analyses in the future (population stars??)!\nData The data used to generate the examples at the top of this page comes from the United States Consumer Product Safety Commission’s National Electronic Injury Surveillance System dataset, which can be found at the following link: https://www.cpsc.gov/Research--Statistics/NEISS-Injury-Data . This dataset is quite rich, containing data gathered from a large number of emergency departments in the US. The overall dataset contains demographic information, including sex, age, and ethnicity, as well as a number of other interesting pieces of information including whether a particular product was involved in the injury (as in the left plot above, which includes all ER visits in the dataset from 2019 which involved a trampoline), and whether drugs or alcohol were involved in the injury. The dataset also includes for each datapoint/injury, a brief narrative outlining the loose details of the incident. The website contains a neat query system where the user can narrow down the dataset by filtering for year, product involved, and other data characteristics.\nFigure-Specific Details I opted to use a population pyramid for a slighly atypical analysis in the , that is, to examine the demographics of patients who visited a number of U.S. emergency rooms in 2019, using the dataset described above. I chose to narrow the dataset to two types of emergency room incidents, to depict two very different-looking population pyramids: the left plot shows the distribution of patients who visited the ER due to trampoline-related accidents, and the right plot shows the same of patients who visited the ER due to incidents involving the use of alcohol.\nI’ll use the general framework for interpreting a population pyramid described above to discuss the information I gleaned from these plots, first analyzing the plot on the left, then the plot on the right.\nTrampoline-Based Injuries As stated, this plot shows a breakdown of the age and sex characteristics of the group of individuals who visited the emergency room due to trampoline accidents in 2019. Prior to generating the plot, I hypothesized that the shape would be heavy at the base, and thinner on the top, as children are typical trampoline users, and I recall from my daredevil youth that I much preferred playing on a trampoline without a protective net. Though as an adult, I still struggle to ignore the siren song of the trampoline, I tend to be more careful than when I was a child. Stereotypes might lead some to believe that male children are more prone to taking trampoline-related risks, but I knew many a reckless young female when I was growing up, so I hypothesized a symmetric pyramid.\nIndeed, this is what the plot shows – a very heavy base, with a peak around 9-10 years old. Around the teenage years, these injuries drop off significantly, although not completely, perhaps in part due to some in the 25-40 year old age group having young children, and being tempted by their child’s trampoline. However, once we hit the mid-40’s, there are very few representatives from that cohort and older in this population. Additionally, the plot is quite symmetric – males and females are more-or-less equally represented in this population. There doesn’t appear to be an interaction of age and sex, as there are no bulges or notches present on one side but not the other.\nAccidents Involving Alcohol A slightly more sobering (sorry!) analysis is on the right – a population pyramid showing the age/sex distribution amongst ER visits resulting from alcohol-related injuries. In this case, I did hypothesize a difference between sexes, with males making up a somewhat larger proportion of the population, though still a fair few females. I also predicted a pear-shaped pyramid, with very few representatives from the 0-20 year old cohort, the bulk of patients in the 20-30 year old range, and a consistent taper down as age increased.\nI was somewhat surprised by the shape of the resulting plot. Although I’d anticipated a plot that was heavier on the male side than the female side, I hadn’t expected the difference to be so dramatic. Almost across the board, each male age cohort has about double (if not more) members than the corresponding female cohort. I was also surprised to see bimodal distributions on both sides, with a bulge from about the mid-teens through around 35, but then an even larger bulge in the older years, peaking around the early-to-mid sixties. Folks getting wild in their retirement years! Though females are represented less in the overall population distribution, these characteristics are still present in that sub-population’s distribution.\nI could see analyses like these being useful for target specific groups to recieve safety training on a particular product, deciding who to design flyers for, which, say, encourage moderation and safety when consuming alcohol, or to determine suggested age ranges for certain play equipment.\nPresentation Tips Color: Typically, this type of visualization uses color in a simple fashion, either using a uniform color and annotating each side with the appropriate label, or selecting two distinct colors to distinguish between one side and the other. Since a population pyramid generally plots the distribution of males versus females, we often see plots that select pink for the female distribution and blue for the male distribution. However, this is a bit of an anachronism in today’s society, as these colors carry the weight of long-term stereotypical gender associations. Some argue that choosing these colors allows for speedy interpretation of the figure, however, if a reader has so little time or mental energy that scanning a legend is unmanagable, then, well, perhaps they should skip looking at the figure altogether. Interestingly, it has been suggested that simply flipping the colors around can be problematic, as it can lead to confusion due to the aforementioned speedy interpretation of those particular colors in a figure.\nSo, I chose to create a graph with different colors altogether, inspired by the color choices in the Telegraph; green for males, and purple for females. A discussion of the motivation behind this choice, as well as more detailed arguments against the selection of blue/pink to distinguish male/female populations can be found at the following link: https://blog.datawrapper.de/gendercolor/ .\nAlthough some may consider it an attractive or interesting design detail to apply a color gratient based on age, this is not a common choice. Not only is it unnecessary to code this using color, since the axis makes the age range quite clear, it could also distract readers.\nFinally, avoid this color choice for this type of visualization at all costs! (But please do use this very attractive 70’s-inspired rainbow palette for your artistic projects!)\n Composition: Since these plots are fairly straight-forward, there are only a few small details to keep in mind regarding composition. The first is that even if the most populous bin in on one side is less than the most populous bin on the other, the x-axis should range from [-n, n], with n corresponding to the most populous bin overall. Although this is a small detail, this places the line dividing the sex/two-factor distributions directly in the middle of the plot, which can aid in accurate comparison of one sub-population versus the other. Also, in my opinion, it is much more visually harmonious.\nAnnotation:\nThe Population Pyramid is a useful visualization in no small part because of the rich possibilities of interpretation without the need for excessive labeling or annotation. However, if a particular feature of the shape of the plot corresponds to, say, a cultural event that is relevant to the context of the figure, it would be acceptable to add a small annotation to the plot, indicating the events related to the feature of the plot. As with all graph annotation, it is better to keep it minimal and clean. Below is an example of a well-annotated population pyramid (although perhaps with a bit much going on, I would personally keep it to one or two annotations):\n Although the annotations/labels add a fair bit of busyness to the figure, the story behind the plot is enriched with the additional details. Note that in addition to the annotations, the figure overlays the outline of a population pyramid from an earlier date on top of the more recent pyramid. This also allows additional temporal information to the figure’s story, which I find to be quite effective.\nIn contrast, here is a poorly annotated population pyramid:\n First of all…that font! But, that is not relevant to the discussion of the pyramid itself. In this case, although there is certainly information added through the annotations, the positioning and quantity of labels distracts from the overall impact of the figure. This quantity of information would be better suited to a narrative setting, alongside the figure, perhaps with numerical annotations indicating the feature of the plot that corresponds with the additional contextual information. However, the lines indicating the positions on the plat that correspond to each annotation are confusing, in that they point directly to one or the other side of the pyramid. This configuration could come across to the viewer as if only one of the sub-populations corresponds to the information in the annotation, e.g. the middle annotation on the right-hand side appears to be referring only to the female side of the pyramid, however, certainly females are not the only humans who will age!\nMethods Below is the code used to create one of the population pyramids, broken up into a chunk that shows the data wrangling process, and then one showing the plot generation, including the ggarrange() call that was used to combine the two plots.\nData Wrangling The data wrangling process was pretty straight-forward after downloading the data from the link provided in the data section, specifically downloading and working with the entire dataset from 2019. The .xlsx file consists of a row corresponding to each emergency room visit, and the columns to each piece of information related to that incident, so I simply had to narrow down the dataset to only include rows where the column alcohol_involved was equal to one. I also filtered out rows with age values higher than 100, as there were some instances where the age column contained a code that was not the actual age in years. I also filtered out rows where the patient’s sex was unknown.\nSince the dataset originally included column names with spaces, a configuration that does not play well with R code, I used the handy janitor::clean_names() function to convert the column names into R-friendly strings. Finally, I replaced the 1/2 indicators with “Male”/“Female”, primarily as a hacky way of making the plot’s legend more attractive, although this could have been done directly during the plotting phase. Finally, I narrowed the dataset down to just the sex and age columns, which is all that was needed for the population pyramids.\nalcohol_data \u0026lt;- read_excel(\u0026quot;neiss2019.xlsx\u0026quot;) %\u0026gt;% janitor::clean_names() %\u0026gt;% filter(alcohol_involved == 1, sex != 0, age \u0026lt; 100) %\u0026gt;% mutate(sex = ifelse(sex == 1, \u0026quot;Male\u0026quot;, \u0026quot;Female\u0026quot;)) %\u0026gt;% select(sex, age)  Plotting The plots I created were adapted from code a couple of handy tutorials; the first two links in the citation section below. The two plots were then set up to display side-by-side using ggarrange(). Each function call is annotated in the R chunk below:\nbooze_pyramid \u0026lt;- ggplot(data = alcohol_data, aes(x = age, fill = sex)) + # Basic ggplot call, # specifying age as the # x-values and the # fill value to be # selected based on the # sex value. geom_histogram(data = subset(alcohol_data, sex == \u0026quot;Male\u0026quot;), # Creating the histogram bins = 35, color = \u0026quot;lightgrey\u0026quot;) + # for the male data. geom_histogram(data = subset(alcohol_data, sex == \u0026quot;Female\u0026quot;),# Creating the histogram for mapping = aes(y = - ..count.. ), # the female data. Note that position = \u0026quot;identity\u0026quot;, # the count/y is mapped to bins = 35, color = \u0026quot;lightgrey\u0026quot;) + # negative values, to flip # the bars. scale_y_continuous(limits = c(-350, 350), labels = abs) + # Manually setting the axis scale_x_continuous(limits = c(0, 100)) + # ranges. coord_flip() + # Flipping the x/y axes. ggtitle(\u0026quot;Involving Alcohol-Use\u0026quot;) + # Tweaking the title and ylab(\u0026quot;Count\u0026quot;) + # other details. theme_minimal() + theme(plot.title = element_text(size = 10, hjust = 0.5)) + scale_fill_manual(values = c(\u0026quot;#80337c\u0026quot;, \u0026quot;#338057\u0026quot;))  The ggarrange() call is included below, as this is a handy function to include several individual plots in a single overall figure.\npyramid_two \u0026lt;- ggarrange(trampoline_pyramid, booze_pyramid, heights = c(2, 2), # Call to ggarrange, ncol = 2, nrow = 1, common.legend = TRUE, align = 'v', # passing in both plot # objects and setting legend = \u0026quot;right\u0026quot;) %\u0026gt;% # display params. # Adding \u0026quot;overall\u0026quot; fig # title. annotate_figure(top = text_grob(\u0026quot;Accidents Resulting in \\nEmergency Room Visits in 2019\\n \u0026quot;, face = \u0026quot;bold\u0026quot;, size = 16)) ## Warning: Removed 2 rows containing missing values (geom_bar). ## Warning: Removed 2 rows containing missing values (geom_bar). ## Warning: Removed 2 rows containing missing values (geom_bar). ## Warning: Removed 2 rows containing missing values (geom_bar). ## Warning: Removed 2 rows containing missing values (geom_bar). ## Warning: Removed 2 rows containing missing values (geom_bar).  Additional Citations Staetsky, Daniel. (2015). Strictly Orthodox Rising: what the demography of British Jews tells us about the future of the community.\nhttps://rpubs.com/walkerke/pyramids_ggplot2 https://stackoverflow.com/questions/14680075/simpler-population-pyramid-in-ggplot2 https://www.cpsc.gov/Research--Statistics/NEISS-Injury-Data https://www.whiteplainspublicschools.org/cms/lib/NY01000029/Centricity/Domain/353/Population%20Pyramids%20Geography.pdf https://populationeducation.org/what-are-different-types-population-pyramids/ https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0 https://datavizcatalogue.com\n","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"e52be429656cfaae17b74ed341e1271c","permalink":"https://vigilant-golick-64a677.netlify.app/post/population-pyramid/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/post/population-pyramid/","section":"post","summary":"A Population Pyramid, also known as an Age and Sex Pyramid is a histogram-like visualization often used to provide a succinct, elegant summary of differences in distribution of groups within a population.","tags":null,"title":"All About the Population Pyramid!","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://vigilant-golick-64a677.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://vigilant-golick-64a677.netlify.app/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://vigilant-golick-64a677.netlify.app/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]